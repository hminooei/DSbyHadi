{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we detect clickbait headlines using a Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is on Kaggle https://www.kaggle.com/amananandrai/clickbait-dataset however you can downloaded the file directly from the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.github.com/hminooei/DSbyHadi/master/data/clickbait_data.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Should I Get Bings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        headline  clickbait\n",
       "0                             Should I Get Bings          1\n",
       "1  Which TV Female Friend Group Do You Belong In          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>31998</td>\n",
       "      <td>Netanyahu Urges Pope Benedict, in Israel, to Denounce Iran</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31999</td>\n",
       "      <td>Computer Makers Prepare to Stake Bigger Claim in Phones</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         headline  clickbait\n",
       "31998  Netanyahu Urges Pope Benedict, in Israel, to Denounce Iran          0\n",
       "31999     Computer Makers Prepare to Stake Bigger Claim in Phones          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "df.head(2)\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd1e93919e8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "percentages = round(df[\"clickbait\"].value_counts()*100/len(df), 1)\n",
    "percentages.plot(kind=\"bar\", title=\"labels' distribution\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the labels are equaly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(pipeline, text_train, label_train, text_val, label_val):\n",
    "    train_preds = pipeline.predict(text_train)\n",
    "    val_preds = pipeline.predict(text_val)\n",
    "    \n",
    "    print(\"train:\")\n",
    "    print(metrics.classification_report(label_train, train_preds, labels=[0, 1], digits=4))\n",
    "    print(metrics.confusion_matrix(label_train, train_preds))\n",
    "    print(\"validation:\")\n",
    "    print(metrics.classification_report(label_val, val_preds, labels=[0, 1], digits=4))\n",
    "    print(metrics.confusion_matrix(label_val, val_preds))\n",
    "\n",
    "\n",
    "def train_measure_model(text_train, label_train, text_val, label_val,\n",
    "                        cv_binary, cv_analyzer, cv_ngram, cv_max_features,\n",
    "                        cv_have_tfidf, cv_use_idf, cfr_penalty, cfr_C, stop_words=None, \n",
    "                        text_column_name=\"headline\"):\n",
    "    cv = CountVectorizer(binary=cv_binary, stop_words=stop_words,\n",
    "                               analyzer=cv_analyzer,\n",
    "                               ngram_range=cv_ngram[1:3],\n",
    "                               max_features=cv_max_features)\n",
    "    if cv_have_tfidf:\n",
    "        pipeline = Pipeline(steps=[(\"vectorizer\", cv), \n",
    "                                   (\"tfidf\", TfidfTransformer(use_idf=cv_use_idf)),\n",
    "                                   (\"classifier\", LogisticRegression(penalty=cfr_penalty,\n",
    "                                                                     C=cfr_C,\n",
    "                                                                     random_state=9,\n",
    "                                                                     max_iter=100,\n",
    "                                                                     n_jobs=None))])\n",
    "    else:\n",
    "        pipeline = Pipeline(steps=[(\"vectorizer\", cv), \n",
    "                                   (\"classifier\", LogisticRegression(penalty=cfr_penalty,\n",
    "                                                                     C=cfr_C,\n",
    "                                                                     random_state=9,\n",
    "                                                                     max_iter=100,\n",
    "                                                                     n_jobs=None))])\n",
    "\n",
    "    pipeline.fit(text_train, label_train)\n",
    "    print_metrics(pipeline, text_train, label_train, text_val, label_val)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_val, text_test, label_train_val, label_test = train_test_split(\n",
    "    df[\"headline\"], \n",
    "    df[\"clickbait\"], \n",
    "    test_size=0.25, \n",
    "    stratify=df[\"clickbait\"], \n",
    "    random_state=9)\n",
    "\n",
    "# Split the train_val dataset to train and validation separete portions.\n",
    "text_train, text_val, label_train, label_val = train_test_split(\n",
    "    text_train_val,\n",
    "    label_train_val, \n",
    "    test_size=0.2, \n",
    "    random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19200,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(19200,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(4800,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(4800,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(8000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(8000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train.shape\n",
    "label_train.shape\n",
    "\n",
    "text_val.shape\n",
    "label_val.shape\n",
    "\n",
    "text_test.shape\n",
    "label_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9631    0.9814    0.9722      9627\n",
      "           1     0.9809    0.9622    0.9715      9573\n",
      "\n",
      "    accuracy                         0.9718     19200\n",
      "   macro avg     0.9720    0.9718    0.9718     19200\n",
      "weighted avg     0.9720    0.9718    0.9718     19200\n",
      "\n",
      "[[9448  179]\n",
      " [ 362 9211]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9501    0.9705    0.9602      2374\n",
      "           1     0.9705    0.9501    0.9602      2426\n",
      "\n",
      "    accuracy                         0.9602      4800\n",
      "   macro avg     0.9603    0.9603    0.9602      4800\n",
      "weighted avg     0.9604    0.9602    0.9602      4800\n",
      "\n",
      "[[2304   70]\n",
      " [ 121 2305]]\n",
      "CPU times: user 2.48 s, sys: 178 ms, total: 2.65 s\n",
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cfr_pipeline = train_measure_model(text_train, label_train, text_val, label_val,\n",
    "                                   cv_binary=False, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                   cv_max_features=50000, cv_have_tfidf=True, cv_use_idf=False, \n",
    "                                   cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9615    0.9773    0.9693      9627\n",
      "           1     0.9767    0.9606    0.9686      9573\n",
      "\n",
      "    accuracy                         0.9690     19200\n",
      "   macro avg     0.9691    0.9689    0.9690     19200\n",
      "weighted avg     0.9691    0.9690    0.9690     19200\n",
      "\n",
      "[[9408  219]\n",
      " [ 377 9196]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9527    0.9760    0.9642      2374\n",
      "           1     0.9759    0.9526    0.9641      2426\n",
      "\n",
      "    accuracy                         0.9642      4800\n",
      "   macro avg     0.9643    0.9643    0.9642      4800\n",
      "weighted avg     0.9644    0.9642    0.9642      4800\n",
      "\n",
      "[[2317   57]\n",
      " [ 115 2311]]\n",
      "CPU times: user 1.32 s, sys: 21.5 ms, total: 1.34 s\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cfr_pipeline = train_measure_model(text_train, label_train, text_val, label_val,\n",
    "                                   cv_binary=False, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                   cv_max_features=5000, cv_have_tfidf=True, cv_use_idf=False, \n",
    "                                   cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9628    0.9780    0.9703      9627\n",
      "           1     0.9775    0.9620    0.9697      9573\n",
      "\n",
      "    accuracy                         0.9700     19200\n",
      "   macro avg     0.9701    0.9700    0.9700     19200\n",
      "weighted avg     0.9701    0.9700    0.9700     19200\n",
      "\n",
      "[[9415  212]\n",
      " [ 364 9209]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9547    0.9760    0.9652      2374\n",
      "           1     0.9760    0.9547    0.9652      2426\n",
      "\n",
      "    accuracy                         0.9652      4800\n",
      "   macro avg     0.9653    0.9653    0.9652      4800\n",
      "weighted avg     0.9654    0.9652    0.9652      4800\n",
      "\n",
      "[[2317   57]\n",
      " [ 110 2316]]\n",
      "CPU times: user 1.35 s, sys: 17.1 ms, total: 1.37 s\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cfr_pipeline = train_measure_model(text_train, label_train, text_val, label_val,\n",
    "                                   cv_binary=True, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                   cv_max_features=5000, cv_have_tfidf=True, cv_use_idf=False, \n",
    "                                   cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9712    0.9843    0.9777      9627\n",
      "           1     0.9840    0.9706    0.9773      9573\n",
      "\n",
      "    accuracy                         0.9775     19200\n",
      "   macro avg     0.9776    0.9775    0.9775     19200\n",
      "weighted avg     0.9776    0.9775    0.9775     19200\n",
      "\n",
      "[[9476  151]\n",
      " [ 281 9292]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9632    0.9815    0.9723      2374\n",
      "           1     0.9815    0.9633    0.9723      2426\n",
      "\n",
      "    accuracy                         0.9723      4800\n",
      "   macro avg     0.9724    0.9724    0.9723      4800\n",
      "weighted avg     0.9725    0.9723    0.9723      4800\n",
      "\n",
      "[[2330   44]\n",
      " [  89 2337]]\n",
      "CPU times: user 1.32 s, sys: 13.6 ms, total: 1.33 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cfr_pipeline = train_measure_model(text_train, label_train, text_val, label_val,\n",
    "                                   cv_binary=True, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                   cv_max_features=5000, cv_have_tfidf=True, cv_use_idf=True, \n",
    "                                   cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_model_on_test(model, text_test, label_test):\n",
    "    test_preds = model.predict(text_test)\n",
    "    test_metrics = metrics.classification_report(label_test, test_preds, labels=[0, 1], digits=4)\n",
    "    print(test_metrics)\n",
    "    print(metrics.confusion_matrix(label_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9613    0.9690    0.9651      4000\n",
      "           1     0.9688    0.9610    0.9649      4000\n",
      "\n",
      "    accuracy                         0.9650      8000\n",
      "   macro avg     0.9650    0.9650    0.9650      8000\n",
      "weighted avg     0.9650    0.9650    0.9650      8000\n",
      "\n",
      "[[3876  124]\n",
      " [ 156 3844]]\n"
     ]
    }
   ],
   "source": [
    "measure_model_on_test(cfr_pipeline, text_test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the best model so far on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_name = 'clickbait-model-sm.pkl'\n",
    "cfr_pipeline.named_steps.vectorizer.stop_words_ = None # to reduce the pickle size (from 5mb to 500kb)\n",
    "pickle.dump(cfr_pipeline, open(model_name, 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find mislabeled samples by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incorrect_predictions(trained_model, all_data, text_df, label_series):\n",
    "    col_name = text_df.columns.values.tolist()[0]\n",
    "    preds = trained_model.predict(text_df[col_name])\n",
    "    incorrectly_predicted = text_df.loc[label_series != preds]\n",
    "    incorrectly_predicted.shape\n",
    "    res = incorrectly_predicted.merge(all_data, on=col_name, suffixes=(\"_left\", \"_right\"))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_added = get_incorrect_predictions(cfr_pipeline, df, text_train.to_frame(name=\"headline\"), label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Big Batch Breakfast Bakes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6 Little Victories</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Meet Stoner Sloth, The NSW Government's New Anti-Pot Mascot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      headline  clickbait\n",
       "0                                    Big Batch Breakfast Bakes          1\n",
       "1                                           6 Little Victories          1\n",
       "2  Meet Stoner Sloth, The NSW Government's New Anti-Pot Mascot          1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(432, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_be_added.head(3)\n",
    "to_be_added.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare and add the mislabeled samples back to the training set (**Manual Boosting**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_text_train = to_be_added[\"headline\"]\n",
    "extra_label_train = to_be_added[\"clickbait\"]\n",
    "\n",
    "extra_label_train = np.array(extra_label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9678    0.9778    0.9728      9778\n",
      "           1     0.9778    0.9677    0.9727      9854\n",
      "\n",
      "    accuracy                         0.9727     19632\n",
      "   macro avg     0.9728    0.9728    0.9727     19632\n",
      "weighted avg     0.9728    0.9727    0.9727     19632\n",
      "\n",
      "[[9561  217]\n",
      " [ 318 9536]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9652    0.9819    0.9735      2374\n",
      "           1     0.9820    0.9654    0.9736      2426\n",
      "\n",
      "    accuracy                         0.9735      4800\n",
      "   macro avg     0.9736    0.9736    0.9735      4800\n",
      "weighted avg     0.9737    0.9735    0.9735      4800\n",
      "\n",
      "[[2331   43]\n",
      " [  84 2342]]\n",
      "test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9648    0.9675    0.9662      4000\n",
      "           1     0.9674    0.9647    0.9661      4000\n",
      "\n",
      "    accuracy                         0.9661      8000\n",
      "   macro avg     0.9661    0.9661    0.9661      8000\n",
      "weighted avg     0.9661    0.9661    0.9661      8000\n",
      "\n",
      "[[3870  130]\n",
      " [ 141 3859]]\n"
     ]
    }
   ],
   "source": [
    "boosted_text_train = pd.concat([text_train, extra_text_train])\n",
    "boosted_label_train = np.concatenate([label_train, extra_label_train], axis=0)\n",
    "\n",
    "cfr_pipeline_1x = train_measure_model(boosted_text_train, boosted_label_train, \n",
    "                                      text_val, label_val,\n",
    "                                      cv_binary=True, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                      cv_max_features=5000, cv_have_tfidf=True, cv_use_idf=True, \n",
    "                                      cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)\n",
    "\n",
    "print(\"test:\")\n",
    "measure_model_on_test(cfr_pipeline_1x, text_test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boost the training set twice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9759    0.9799    0.9779      9929\n",
      "           1     0.9802    0.9763    0.9783     10135\n",
      "\n",
      "    accuracy                         0.9781     20064\n",
      "   macro avg     0.9781    0.9781    0.9781     20064\n",
      "weighted avg     0.9781    0.9781    0.9781     20064\n",
      "\n",
      "[[9729  200]\n",
      " [ 240 9895]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9656    0.9806    0.9730      2374\n",
      "           1     0.9807    0.9658    0.9732      2426\n",
      "\n",
      "    accuracy                         0.9731      4800\n",
      "   macro avg     0.9732    0.9732    0.9731      4800\n",
      "weighted avg     0.9732    0.9731    0.9731      4800\n",
      "\n",
      "[[2328   46]\n",
      " [  83 2343]]\n",
      "test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9672    0.9655    0.9663      4000\n",
      "           1     0.9656    0.9673    0.9664      4000\n",
      "\n",
      "    accuracy                         0.9664      8000\n",
      "   macro avg     0.9664    0.9664    0.9664      8000\n",
      "weighted avg     0.9664    0.9664    0.9664      8000\n",
      "\n",
      "[[3862  138]\n",
      " [ 131 3869]]\n"
     ]
    }
   ],
   "source": [
    "boosted_text_train_2x = pd.concat([text_train]+[extra_text_train]*2)\n",
    "boosted_label_train_2x = np.concatenate([label_train]+[extra_label_train]*2, axis=0)\n",
    "\n",
    "cfr_pipeline_2x = train_measure_model(boosted_text_train_2x, boosted_label_train_2x, \n",
    "                                      text_val, label_val,\n",
    "                                      cv_binary=True, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                      cv_max_features=5000, cv_have_tfidf=True, cv_use_idf=True, \n",
    "                                      cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)\n",
    "\n",
    "print(\"test:\")\n",
    "measure_model_on_test(cfr_pipeline_2x, text_test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the 2x manual boosted model has the best metrics. \n",
    "\n",
    "Below, you can see the effect of overfitting in the 5x version!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9802    0.9812    0.9807     10080\n",
      "           1     0.9818    0.9808    0.9813     10416\n",
      "\n",
      "    accuracy                         0.9810     20496\n",
      "   macro avg     0.9810    0.9810    0.9810     20496\n",
      "weighted avg     0.9810    0.9810    0.9810     20496\n",
      "\n",
      "[[ 9891   189]\n",
      " [  200 10216]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9659    0.9777    0.9717      2374\n",
      "           1     0.9779    0.9662    0.9720      2426\n",
      "\n",
      "    accuracy                         0.9719      4800\n",
      "   macro avg     0.9719    0.9719    0.9719      4800\n",
      "weighted avg     0.9719    0.9719    0.9719      4800\n",
      "\n",
      "[[2321   53]\n",
      " [  82 2344]]\n",
      "test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9681    0.9640    0.9661      4000\n",
      "           1     0.9642    0.9683    0.9662      4000\n",
      "\n",
      "    accuracy                         0.9661      8000\n",
      "   macro avg     0.9661    0.9661    0.9661      8000\n",
      "weighted avg     0.9661    0.9661    0.9661      8000\n",
      "\n",
      "[[3856  144]\n",
      " [ 127 3873]]\n"
     ]
    }
   ],
   "source": [
    "boosted_text_train_3x = pd.concat([text_train]+[extra_text_train]*3)\n",
    "boosted_label_train_3x = np.concatenate([label_train]+[extra_label_train]*3, axis=0)\n",
    "\n",
    "cfr_pipeline_3x = train_measure_model(boosted_text_train_3x, boosted_label_train_3x, \n",
    "                                      text_val, label_val,\n",
    "                                      cv_binary=True, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                      cv_max_features=5000, cv_have_tfidf=True, cv_use_idf=True, \n",
    "                                      cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)\n",
    "\n",
    "print(\"test:\")\n",
    "measure_model_on_test(cfr_pipeline_3x, text_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9852    0.9849    0.9851     10231\n",
      "           1     0.9856    0.9859    0.9857     10697\n",
      "\n",
      "    accuracy                         0.9854     20928\n",
      "   macro avg     0.9854    0.9854    0.9854     20928\n",
      "weighted avg     0.9854    0.9854    0.9854     20928\n",
      "\n",
      "[[10077   154]\n",
      " [  151 10546]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9662    0.9764    0.9713      2374\n",
      "           1     0.9767    0.9666    0.9716      2426\n",
      "\n",
      "    accuracy                         0.9715      4800\n",
      "   macro avg     0.9715    0.9715    0.9715      4800\n",
      "weighted avg     0.9715    0.9715    0.9715      4800\n",
      "\n",
      "[[2318   56]\n",
      " [  81 2345]]\n",
      "test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9678    0.9633    0.9655      4000\n",
      "           1     0.9634    0.9680    0.9657      4000\n",
      "\n",
      "    accuracy                         0.9656      8000\n",
      "   macro avg     0.9656    0.9656    0.9656      8000\n",
      "weighted avg     0.9656    0.9656    0.9656      8000\n",
      "\n",
      "[[3853  147]\n",
      " [ 128 3872]]\n"
     ]
    }
   ],
   "source": [
    "boosted_text_train_4x = pd.concat([text_train]+[extra_text_train]*4)\n",
    "boosted_label_train_4x = np.concatenate([label_train]+[extra_label_train]*4, axis=0)\n",
    "\n",
    "cfr_pipeline_4x = train_measure_model(boosted_text_train_4x, boosted_label_train_4x, \n",
    "                                      text_val, label_val,\n",
    "                                      cv_binary=True, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                      cv_max_features=5000, cv_have_tfidf=True, cv_use_idf=True, \n",
    "                                      cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)\n",
    "\n",
    "print(\"test:\")\n",
    "measure_model_on_test(cfr_pipeline_4x, text_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9892    0.9867    0.9879     10382\n",
      "           1     0.9875    0.9898    0.9886     10978\n",
      "\n",
      "    accuracy                         0.9883     21360\n",
      "   macro avg     0.9883    0.9883    0.9883     21360\n",
      "weighted avg     0.9883    0.9883    0.9883     21360\n",
      "\n",
      "[[10244   138]\n",
      " [  112 10866]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9658    0.9768    0.9713      2374\n",
      "           1     0.9771    0.9662    0.9716      2426\n",
      "\n",
      "    accuracy                         0.9715      4800\n",
      "   macro avg     0.9715    0.9715    0.9715      4800\n",
      "weighted avg     0.9715    0.9715    0.9715      4800\n",
      "\n",
      "[[2319   55]\n",
      " [  82 2344]]\n",
      "test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9698    0.9635    0.9666      4000\n",
      "           1     0.9637    0.9700    0.9669      4000\n",
      "\n",
      "    accuracy                         0.9667      8000\n",
      "   macro avg     0.9668    0.9667    0.9667      8000\n",
      "weighted avg     0.9668    0.9667    0.9667      8000\n",
      "\n",
      "[[3854  146]\n",
      " [ 120 3880]]\n"
     ]
    }
   ],
   "source": [
    "boosted_text_train_5x = pd.concat([text_train]+[extra_text_train]*5)\n",
    "boosted_label_train_5x = np.concatenate([label_train]+[extra_label_train]*5, axis=0)\n",
    "\n",
    "cfr_pipeline_5x = train_measure_model(boosted_text_train_5x, boosted_label_train_5x, \n",
    "                                      text_val, label_val,\n",
    "                                      cv_binary=True, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                      cv_max_features=5000, cv_have_tfidf=True, cv_use_idf=True, \n",
    "                                      cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)\n",
    "\n",
    "print(\"test:\")\n",
    "measure_model_on_test(cfr_pipeline_5x, text_test, label_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
